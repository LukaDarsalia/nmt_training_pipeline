# Baseline MarianMT configuration with custom tokenizers

dataset:
  train_file: "train.parquet"
  valid_file: "valid.parquet"
  test_file: "test.parquet"

model:
  architecture: "marian_custom"
  name: "Helsinki-NLP/opus-mt-en-de"
  encoder_tokenizer: "RichNachos/georgian-corpus-tokenizer-test"
  decoder_tokenizer: "Helsinki-NLP/opus-mt-en-de"
  pretrained: false

trainer:
  trainer_type: "seq2seq_trainer"
  output_dir: "marian_baseline_output"
  num_train_epochs: 1
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 1
  learning_rate: 5e-5
  weight_decay: 0.0
  warmup_steps: 0
  lr_scheduler_type: "cosine"
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  logging_steps: 100
  max_source_length: 128
  max_target_length: 128
  early_stopping_patience: 3
  early_stopping_threshold: 0.0
  metrics:
    - bleu
