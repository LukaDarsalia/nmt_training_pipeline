# Baseline Marian Model with Encoder-Decoder Tokenizer
# This configuration uses EncoderDecoderTokenizer with:
# - Encoder: openai-community/gpt2
# - Decoder: RichNachos/georgian-corpus-tokenizer-test tokenizer

# Tokenizer configuration
tokenizer:
  type: "encoder_decoder"  # Use encoder-decoder tokenizer
  encoder_model: "roberta-base"
  decoder_model: "RichNachos/georgian-corpus-tokenizer-test"

# Model configuration
model:
  type: "marian_custom"  # Use custom Marian architecture
  # Model architecture parameters
  architecture:
    d_model: 512
    encoder_layers: 4
    decoder_layers: 4
    encoder_ffn_dim: 768
    decoder_ffn_dim: 768
    encoder_attention_heads: 8
    decoder_attention_heads: 8
    dropout: 0.1
    activation_function: "swish"

  # Generation configuration
  generation_config:
    max_length: 128
    num_beams: 1
    early_stopping: true
    do_sample: false

# Data configuration
data:
  max_length: 128
  source_column: "en"
  target_column: "ka"
  target_prefix: ""
  encoder_decoder_preprocessing: true  # Enable special encoder-decoder tokenization
  max_source_length: 128
  max_target_length: 128
  preprocessing_num_workers: 4

# Trainer configuration
trainer:
  type: "seq2seq_with_metrics"

  # Training parameters
  training:
    num_epochs: 20
    train_batch_size: 32
    eval_batch_size: 32
    gradient_accumulation_steps: 1

    # Learning rate and optimization
    learning_rate: 5e-4
    lr_scheduler_type: "cosine"
    warmup_ratio: 0.1
    weight_decay: 0.01

    # Evaluation and logging
    evaluation_strategy: "steps"
    eval_steps: 0.2
    logging_steps: 0.2
    save_steps: 0.2
    save_total_limit: 3

    # Other parameters
    fp16: true
    num_workers: 4
    seed: 42

    # Best model tracking - using COMET as the main metric
    metric_for_best_model: "eval_georgian_comet_comet"
    greater_is_better: true

    # Label smoothing
    label_smoothing_factor: 0.1

    # Reporting
    report_to: "wandb"

    # Early stopping
    early_stopping:
      enabled: true
      patience: 3
      threshold: 0.001

    # Enhanced prediction logging with comprehensive evaluation results
    prediction_logging:
      enabled: true
      num_samples: 30  # Log more samples for better insights
      frequency: 1     # Log every evaluation for detailed tracking

  # Generation parameters for evaluation
  generation:
    max_length: 128
    num_beams: 1
    eval_num_beams: 1

  # Evaluation metrics - Only SacreBLEU, chrF++, and Georgian COMET
  evaluation:
    evaluators:
      - name: "sacrebleu"
        config: {}
      - name: "chrf"
        config:
          word_order: 2  # chrF++
      - name: "georgian_comet"
        config:
          model_name: "Darsala/georgian_comet"
          batch_size: 32
          device: "cuda"
          gpus: 1

# Experiment metadata
experiment:
  name: "marian_encoder_decoder_tokenizer"
  description: "Baseline Marian with EncoderDecoderTokenizer (XLM encoder + Georgian corpus decoder) - Enhanced WandB logging"
  tags: ["marian", "encoder_decoder_tokenizer", "xlm", "georgian", "baseline", "enhanced_logging"] 