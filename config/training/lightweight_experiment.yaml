# Lightweight Experiment Configuration
# Fast training configuration for testing and development

# Model configuration
model:
  type: "marian_custom"

  # Smaller architecture for faster training
  architecture:
    d_model: 256
    encoder_layers: 3
    decoder_layers: 3
    encoder_ffn_dim: 1024
    decoder_ffn_dim: 1024
    encoder_attention_heads: 4
    decoder_attention_heads: 4
    dropout: 0.1

  generation_config:
    max_length: 64  # Shorter sequences
    num_beams: 1

# Data configuration
data:
  max_length: 64  # Shorter for speed
  source_column: "en"
  target_column: "ka"

# Trainer configuration
trainer:
  type: "standard_seq2seq"  # Simple trainer

  training:
    num_epochs: 3  # Few epochs for testing
    train_batch_size: 64  # Larger batch for speed
    eval_batch_size: 64
    gradient_accumulation_steps: 1

    learning_rate: 1e-3  # Higher LR for faster convergence
    lr_scheduler_type: "linear"
    warmup_ratio: 0.05
    weight_decay: 0.01

    evaluation_strategy: "steps"
    eval_steps: 100  # Frequent evaluation
    logging_steps: 25
    save_steps: 100

    fp16: true
    num_workers: 2
    seed: 42

    metric_for_best_model: "eval_loss"
    greater_is_better: false

    early_stopping:
      enabled: true
      patience: 2  # Quick stopping

    prediction_logging:
      enabled: true
      num_samples: 5  # Fewer samples
      frequency: 1

  generation:
    max_length: 64
    num_beams: 1
    eval_num_beams: 3  # Fewer beams for speed

# Experiment metadata
experiment:
  name: "lightweight_test"
  description: "Fast training configuration for testing"
  tags: ["lightweight", "test", "development"]