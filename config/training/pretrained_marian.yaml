# Pretrained Marian Model Fine-tuning Configuration
# This configuration fine-tunes a pretrained multilingual model

# Model configuration
model:
  type: "marian_pretrained"  # Use pretrained Marian model
  # Valid option 1: Use Facebook's M2M100 (supports English to Georgian)
  model_name: "facebook/m2m100_418M"  # Multilingual model with Georgian support

  # Alternative options (uncomment to use):
  # model_name: "facebook/mbart-large-50-many-to-many-mmt"  # mBART with Georgian (ka_GE)
  # model_name: "alirezamsh/small100"  # Smaller multilingual model with Georgian

  # Note: For M2M100, we need to set forced_bos_token_id for target language
  # Georgian language code is "ka" in M2M100
  target_lang: "ka"  # Target language code for M2M100

  # Generation configuration
  generation_config:
    max_length: 128
    num_beams: 1
    early_stopping: true
    do_sample: false
    length_penalty: 1.0

# Data configuration
data:
  max_length: 128
  source_column: "en"
  target_column: "ka"
  target_prefix: ""
  # Special handling for multilingual models
  multilingual_model: true

# Trainer configuration
trainer:
  type: "seq2seq_with_metrics"

  # Training parameters
  training:
    num_epochs: 5  # Fewer epochs for fine-tuning
    train_batch_size: 16
    eval_batch_size: 16
    gradient_accumulation_steps: 2

    # Learning rate and optimization
    learning_rate: 2e-5  # Lower learning rate for fine-tuning
    lr_scheduler_type: "cosine"
    warmup_ratio: 0.05  # Less warmup for fine-tuning
    weight_decay: 0.01

    # Evaluation and logging
    evaluation_strategy: "steps"
    eval_steps: 250
    logging_steps: 50
    save_steps: 250
    save_total_limit: 3

    # Other parameters
    fp16: true
    num_workers: 4
    seed: 42

    # Best model tracking
    metric_for_best_model: "eval_sacrebleu"
    greater_is_better: true

    # Label smoothing
    label_smoothing_factor: 0.05  # Less smoothing for fine-tuning

    # Reporting
    report_to: "wandb"

    # Early stopping
    early_stopping:
      enabled: true
      patience: 2
      threshold: 0.001

    # Prediction logging
    prediction_logging:
      enabled: true
      num_samples: 20
      frequency: 1  # More frequent logging

  # Generation parameters for evaluation
  generation:
    max_length: 128
    num_beams: 1
    eval_num_beams: 5

  # Evaluation metrics - Only SacreBLEU, chrF++, and Georgian COMET
  evaluation:
    evaluators:
      - name: "sacrebleu"
        config: {}
      - name: "chrf"
        config:
          word_order: 2  # chrF++
      - name: "georgian_comet"
        config:
          model_name: "Darsala/georgian_comet"  # Your fine-tuned Georgian COMET model
          batch_size: 16
          device: "cuda"
          gpus: 1

# Experiment metadata
experiment:
  name: "pretrained_multilingual_finetune"
  description: "Fine-tuning pretrained multilingual model (M2M100) for ENâ†’KA translation"
  tags: ["fine-tuning", "multilingual", "m2m100", "georgian"]