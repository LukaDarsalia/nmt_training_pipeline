# M2M100 Model with Pretrained Weights
# This configuration uses the pretrained M2M100 model with its own tokenizer

# Tokenizer configuration
tokenizer:
  type: "m2m100_multilingual"  # Use M2M100 multilingual tokenizer

# Model configuration
model:
  type: "m2m100_pretrained"  # Use pretrained M2M100 model
  model_name: "facebook/m2m100_418M"  # Pretrained M2M100 model

  # Generation configuration
  generation_config:
    max_length: 128
    num_beams: 5
    early_stopping: true
    do_sample: false
    forced_bos_token_id: 250004  # English token ID for M2M100

# Data configuration
data:
  max_length: 128
  source_column: "en"
  target_column: "ka"
  target_prefix: ""
  source_lang: "en"  # Source language for M2M100
  target_lang: "ka"  # Target language for M2M100

# Trainer configuration
trainer:
  type: "seq2seq_with_metrics"

  # Training parameters
  training:
    num_epochs: 8
    train_batch_size: 4
    eval_batch_size: 4
    gradient_accumulation_steps: 8

    # Learning rate and optimization - lower for pretrained model
    learning_rate: 1e-5
    lr_scheduler_type: "cosine"
    warmup_ratio: 0.1
    weight_decay: 0.01

    # Evaluation and logging
    evaluation_strategy: "steps"
    eval_steps: 0.2
    logging_steps: 0.2
    save_steps: 0.2
    save_total_limit: 3

    # Other parameters
    fp16: true
    num_workers: 4
    seed: 42

    # Best model tracking - using COMET as the main metric
    metric_for_best_model: "eval_georgian_comet_comet"
    greater_is_better: true

    # Label smoothing
    label_smoothing_factor: 0.1

    # Reporting
    report_to: "wandb"

    # Early stopping
    early_stopping:
      enabled: true
      patience: 3
      threshold: 0.001

    # Prediction logging
    prediction_logging:
      enabled: true
      num_samples: 20
      frequency: 2

  # Generation parameters for evaluation
  generation:
    max_length: 512
    num_beams: 5
    eval_num_beams: 5

  # Evaluation metrics - Only SacreBLEU, chrF++, and Georgian COMET
  evaluation:
    evaluators:
      - name: "sacrebleu"
        config: {}
      - name: "chrf"
        config:
          word_order: 2  # chrF++
      - name: "georgian_comet"
        config:
          model_name: "Darsala/georgian_comet"
          batch_size: 16
          device: "cuda"
          gpus: 1

# Experiment metadata
experiment:
  name: "m2m100_pretrained"
  description: "M2M100 pretrained model with fine-tuning for ENâ†’KA translation"
  tags: ["m2m100", "pretrained", "multilingual", "facebook", "fine_tuning"] 