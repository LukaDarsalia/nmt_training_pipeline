# Encoder-Decoder Model Training Configuration
# This configuration uses BERT encoder with GPT-2 decoder

# Model configuration
model:
  type: "encoder_decoder_pretrained"  # Use encoder-decoder combination
  encoder_model: "bert-base-uncased"
  decoder_model: "gpt2"
  encoder_decoder_preprocessing: true  # Enable special preprocessing

  # Generation configuration
  generation_config:
    max_length: 128
    num_beams: 1
    early_stopping: true
    do_sample: false
    pad_token_id: 50256  # GPT-2 pad token

# Data configuration
data:
  max_length: 128
  source_column: "en"
  target_column: "ka"
  target_prefix: ""
  encoder_decoder_preprocessing: true  # Enable special tokenization

# Trainer configuration
trainer:
  type: "standard_seq2seq"  # Use standard trainer for encoder-decoder

  # Training parameters
  training:
    num_epochs: 15
    train_batch_size: 16
    eval_batch_size: 16
    gradient_accumulation_steps: 4

    # Learning rate and optimization
    learning_rate: 1e-4
    lr_scheduler_type: "cosine"
    warmup_ratio: 0.1
    weight_decay: 0.05

    # Evaluation and logging
    evaluation_strategy: "steps"
    eval_steps: 500
    logging_steps: 100
    save_steps: 500
    save_total_limit: 3

    # Other parameters
    fp16: true
    num_workers: 8
    seed: 123

    # Best model tracking
    metric_for_best_model: "eval_loss"
    greater_is_better: false

    # Label smoothing
    label_smoothing_factor: 0.1

    # Reporting
    report_to: "wandb"

    # Early stopping
    early_stopping:
      enabled: true
      patience: 2
      threshold: 0.0

    # Prediction logging
    prediction_logging:
      enabled: true
      num_samples: 100
      frequency: 1

  # Generation parameters for evaluation
  generation:
    max_length: 128
    num_beams: 1
    eval_num_beams: 5

# Experiment metadata
experiment:
  name: "bert_gpt2_encoder_decoder"
  description: "BERT encoder with GPT-2 decoder for ENâ†’KA translation"
  tags: ["encoder-decoder", "bert", "gpt2", "experimental"]